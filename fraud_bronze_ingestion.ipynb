{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15acedb6-2bfa-407e-a72f-9ab9e28093e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# Fraud Detection - Bronze Layer Ingestion\n",
    "\n",
    "This notebook ingests raw fraud detection data into the Bronze layer of the medallion architecture.\n",
    "\n",
    "## Data Sources\n",
    "* **transactions_data.csv** - Transaction records\n",
    "* **cards_data.csv** - Card information\n",
    "* **users_data.csv** - User/customer data\n",
    "* **mcc_codes.json** - Merchant Category Code descriptions\n",
    "* **train_fraud_labels.json** - Fraud labels for training data\n",
    "\n",
    "## Output Tables\n",
    "* `workspace.fraud.transactions_bronze` - All transactions (train/score split happens in Silver layer)\n",
    "* `workspace.fraud.cards_bronze` - Card dimension (sensitive fields removed)\n",
    "* `workspace.fraud.users_bronze` - User dimension\n",
    "* `workspace.fraud.mcc_dim_bronze` - MCC dimension\n",
    "* `workspace.fraud.labels_map` - Transaction ID to fraud label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab40ec64-1ff1-438b-81a0-aedfdbf239e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS workspace.fraud;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107de7fe-a61b-4e04-aa9f-4086e63c2c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd87ac7-cb3a-4985-9ee9-6f132598fc9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62eb788e-9f81-4bbb-a640-fd19d053993b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW SCHEMAS IN workspace;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dffbb2c-361e-44af-af7a-6ddbc5b855c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.fraud.raw;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8412ce7d-32a5-435c-95ec-874f93adc223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW VOLUMES IN workspace.fraud;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db55bb80-f310-46cb-9c80-88d7fa26cf26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 1. Setup\n",
    "Created catalog schema and volume for raw data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d6e9c3-0540-4551-8da5-ca76d661190c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/Workspace/Users/hiverin@gmail.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b83ce2-0ab5-4e82-ad4f-94bbd2916c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 2. Load Raw Data\n",
    "Load all raw CSV and JSON files from the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e24606-8294-45f8-b230-3ef61760e2b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load CSV files"
    }
   },
   "outputs": [],
   "source": [
    "# Define base path for raw data\n",
    "base_path = \"dbfs:/Volumes/workspace/fraud/raw/\"\n",
    "\n",
    "# Load CSV files\n",
    "cards = spark.read.csv(base_path + \"cards_data.csv\", header=True, inferSchema=True)\n",
    "users = spark.read.csv(base_path + \"users_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Loaded {cards.count():,} cards\")\n",
    "print(f\"Loaded {users.count():,} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01ef4ca4-b9a7-4387-a6f3-fdea59179c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 Parse Complex JSON Files\n",
    "Parse nested JSON structures for fraud labels and MCC codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a67179da-018b-4453-911d-6665fe36fc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "Parse fraud labels JSON"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import MapType, StringType, StructType, StructField\n",
    "\n",
    "# Parse train_fraud_labels.json which has structure: {\"target\": {\"tx_id\": \"label\", ...}}\n",
    "schema = StructType([StructField('target', MapType(StringType(), StringType()), True)])\n",
    "\n",
    "labels_raw = (\n",
    "    spark.read\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .schema(schema)\n",
    "    .json(base_path + \"train_fraud_labels.json\")\n",
    ")\n",
    "\n",
    "# Explode the map into rows with tx_id and label columns\n",
    "labels_map = (\n",
    "    labels_raw\n",
    "    .select(F.explode(F.map_entries(F.col(\"target\"))).alias(\"kv\"))\n",
    "    .select(\n",
    "        F.col(\"kv.key\").alias(\"tx_id\"),\n",
    "        F.col(\"kv.value\").alias(\"label\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Loaded {labels_map.count():,} fraud labels\")\n",
    "labels_map.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931424a8-e6ec-495c-9cfe-22ee0ceab914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labels_map.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"workspace.fraud.labels_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3bd2cd-e259-4d21-920c-82c16dcfcdb0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load transactions and create bronze table"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load raw transactions\n",
    "transactions = spark.read.csv(\n",
    "    base_path + \"transactions_data.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded {transactions.count():,} total transactions\")\n",
    "\n",
    "# Add tx_id and tx_ts columns (no split, no join with labels)\n",
    "transactions_bronze = transactions.withColumn(\"tx_id\", F.col(\"id\").cast(\"string\")) \\\n",
    "                                  .withColumn(\"tx_ts\", F.col(\"date\"))\n",
    "\n",
    "# Save to single Bronze table\n",
    "transactions_bronze.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\") \\\n",
    "    .saveAsTable(\"workspace.fraud.transactions_bronze\")\n",
    "\n",
    "print(f\"Created transactions_bronze with {transactions_bronze.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab0779e-1ba4-4962-b169-95581aab2ae6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bronze Tables Section"
    }
   },
   "source": [
    "---\n",
    "## 3. Create Bronze Layer Tables\n",
    "Save raw data to Delta tables (immutable, no transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0498215b-694c-4499-819d-107193a9e9e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parse MCC codes JSON"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Parse mcc_codes.json which has structure: {\"code\": \"description\", ...}\n",
    "# Read line-by-line and combine into single JSON string\n",
    "lines = spark.read.text(base_path + \"mcc_codes.json\")\n",
    "\n",
    "json_df = lines.agg(\n",
    "    F.concat_ws(\"\\n\", F.collect_list(\"value\")).alias(\"json_str\")\n",
    ")\n",
    "\n",
    "# Parse JSON string into a Map and explode into rows\n",
    "mcc_map = json_df.select(\n",
    "    F.from_json(\"json_str\", \"map<string,string>\").alias(\"m\")\n",
    ")\n",
    "\n",
    "mcc_dim = (\n",
    "    mcc_map\n",
    "    .select(F.explode(F.map_entries(\"m\")).alias(\"kv\"))\n",
    "    .select(\n",
    "        F.col(\"kv.key\").alias(\"mcc\"),\n",
    "        F.col(\"kv.value\").alias(\"mcc_description\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Loaded {mcc_dim.count():,} MCC codes\")\n",
    "mcc_dim.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1306b996-e618-450f-a9c5-5eee44aadd0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create cards_bronze table"
    }
   },
   "outputs": [],
   "source": [
    "# Drop sensitive fields (card_number, cvv) before saving to bronze\n",
    "cards_bronze = cards.drop(\"card_number\", \"cvv\")\n",
    "\n",
    "cards_bronze.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"workspace.fraud.cards_bronze\")\n",
    "print(f\"Created cards_bronze with {cards_bronze.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c93ca0-4507-48b3-a3d8-d3886a1211ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create users_bronze table"
    }
   },
   "outputs": [],
   "source": [
    "users_bronze = users\n",
    "\n",
    "users_bronze.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"workspace.fraud.users_bronze\")\n",
    "print(f\"Created users_bronze with {users_bronze.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9ba662-2d15-43fa-a02d-beb566d2f481",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create mcc_dim_bronze table"
    }
   },
   "outputs": [],
   "source": [
    "mcc_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"workspace.fraud.mcc_dim_bronze\")\n",
    "print(f\"Created mcc_dim_bronze with {mcc_dim.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb564d1-792f-4b5f-816d-7ae87694405f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 21"
    }
   },
   "source": [
    "---\n",
    "## 4. Verify Bronze Tables\n",
    "Confirm all tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6089807-aca0-4d9a-b765-c82a377137be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify transactions bronze table"
    }
   },
   "outputs": [],
   "source": [
    "# Verify transactions bronze table\n",
    "print(\"Transactions Bronze Table:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tx_count = spark.sql(\"SELECT COUNT(*) as count FROM workspace.fraud.transactions_bronze\").collect()[0]['count']\n",
    "print(f\"transactions_bronze: {tx_count:>10,} rows\")\n",
    "print()\n",
    "\n",
    "# Show sample from table\n",
    "print(\"Sample from transactions_bronze:\")\n",
    "spark.sql(\"SELECT * FROM workspace.fraud.transactions_bronze LIMIT 3\").show(truncate=False)\n",
    "\n",
    "# Show schema\n",
    "print(\"Schema:\")\n",
    "spark.sql(\"DESCRIBE TABLE workspace.fraud.transactions_bronze\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61ddab9-9c81-42ea-8e0c-a4f6e07bfc4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify MCC dimension"
    }
   },
   "outputs": [],
   "source": [
    "# Verify MCC dimension table\n",
    "print(\"MCC Dimension:\")\n",
    "spark.sql(\"SELECT COUNT(*) as row_count FROM workspace.fraud.mcc_dim_bronze\").show()\n",
    "spark.sql(\"SELECT * FROM workspace.fraud.mcc_dim_bronze LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d86c3d-bbd7-4afe-bd67-a73443d07363",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List all bronze tables"
    }
   },
   "outputs": [],
   "source": [
    "# List all bronze tables in the fraud schema\n",
    "print(\"Bronze Layer Tables:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bronze_tables = [\n",
    "    \"transactions_bronze\",\n",
    "    \"cards_bronze\",\n",
    "    \"users_bronze\",\n",
    "    \"mcc_dim_bronze\",\n",
    "    \"labels_map\"\n",
    "]\n",
    "\n",
    "for table in bronze_tables:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM workspace.fraud.{table}\").collect()[0]['cnt']\n",
    "        print(f\"{table:25} {count:>15,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"{table:25} ERROR: Table not found\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8168936644009259,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fraud_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
